{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b72fef-7eee-4749-a156-33ed4898a67a",
   "metadata": {},
   "source": [
    "### Q1. R-squared in Linear Regression:\n",
    "\n",
    "- **Concept:** R-squared (Coefficient of Determination) measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "- **Calculation:** \\( R^2 = 1 - \\frac{\\text{Sum of Squared Residuals}}{\\text{Total Sum of Squares}} \\)\n",
    "- **Interpretation:** Represents the goodness of fit; values closer to 1 indicate a better fit.\n",
    "\n",
    "### Q2. Adjusted R-squared:\n",
    "\n",
    "- **Definition:** Adjusted R-squared adjusts the R-squared value based on the number of predictors in the model.\n",
    "- **Calculation:** \\( \\text{Adjusted R}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{(n - k - 1)} \\)\n",
    "- **Difference:** Adjusted R-squared penalizes the inclusion of irrelevant predictors, providing a more realistic assessment.\n",
    "\n",
    "### Q3. Appropriate Use of Adjusted R-squared:\n",
    "\n",
    "- **When to Use:** Adjusted R-squared is more appropriate when comparing models with different numbers of predictors.\n",
    "- **Advantage:** It accounts for model complexity, preventing overestimation of model performance.\n",
    "\n",
    "### Q4. RMSE, MSE, and MAE:\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** \\(\\sqrt{\\frac{\\text{Sum of Squared Errors}}{n}}\\)\n",
    "- **MSE (Mean Squared Error):** \\(\\frac{\\text{Sum of Squared Errors}}{n}\\)\n",
    "- **MAE (Mean Absolute Error):** \\(\\frac{\\text{Sum of Absolute Errors}}{n}\\)\n",
    "- **Interpretation:** All metrics measure the average magnitude of errors in predictions.\n",
    "\n",
    "### Q5. Advantages and Disadvantages of Evaluation Metrics:\n",
    "\n",
    "- **Advantages:**\n",
    "  - **RMSE:** Sensitive to large errors, suitable when errors should be penalized.\n",
    "  - **MAE:** Robust to outliers, provides a simpler interpretation.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - **RMSE:** Sensitive to outliers, can be influenced by large errors.\n",
    "  - **MAE:** Ignores the magnitude of errors, less informative.\n",
    "\n",
    "### Q6. Lasso Regularization:\n",
    "\n",
    "- **Concept:** Lasso adds a penalty term (L1 regularization) to the linear regression cost function, forcing some coefficients to become exactly zero.\n",
    "- **Difference from Ridge:** Lasso tends to produce sparse models by eliminating some coefficients entirely.\n",
    "- **Appropriate Use:** When feature selection is essential, as Lasso can lead to a more interpretable model.\n",
    "\n",
    "### Q7. Preventing Overfitting with Regularized Linear Models:\n",
    "\n",
    "- **Mechanism:** Regularization introduces penalty terms for large coefficients, preventing overemphasis on specific features.\n",
    "- **Example:** In a dataset with numerous features, regularization helps avoid overfitting by discouraging the model from assigning excessive importance to noise.\n",
    "\n",
    "### Q8. Limitations of Regularized Linear Models:\n",
    "\n",
    "- **Trade-offs:** Regularization introduces a trade-off between fitting the data well and maintaining simplicity.\n",
    "- **Not Always Best:** Regularized models may not be the best choice if the data is truly complex, and feature importance needs to be accurately captured.\n",
    "\n",
    "### Q9. Model Comparison with Different Metrics:\n",
    "\n",
    "- **Choice:** The choice depends on the specific goals of the analysis. RMSE penalizes large errors more, while MAE is robust to outliers.\n",
    "- **Limitations:** The choice of metric should align with the problem context, and the limitations of each metric should be considered.\n",
    "\n",
    "### Q10. Model Comparison with Regularization Methods:\n",
    "\n",
    "- **Choice:** The choice depends on the specific characteristics of the dataset and the desired properties of the model.\n",
    "- **Trade-offs:** Ridge regularization tends to shrink coefficients but not eliminate them, while Lasso can lead to sparsity. The choice involves a trade-off between feature importance and sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0fa411-06f5-466c-8583-d285b41b80db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
